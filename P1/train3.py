# Archivo: train_improved.py
"""
Script de entrenamiento OPTIMIZADO para aprendizaje r√°pido.
Con 20,000 timesteps deber√≠a mostrar progreso claro en 30-60 minutos.
"""

from robobopy.Robobo import Robobo
from env3 import RoboboEnv
from robobosim.RoboboSim import RoboboSim
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
import os
import numpy as np

# ==================== CONFIGURACI√ìN ====================
ROBOBO_IP = 'localhost'
MODEL_NAME = "ppo_robobo_v2"
LOGS_DIR = "logs/"
MODELS_DIR = "models/"
TOTAL_TIMESTEPS = 20_000  # ‚Üê OPTIMIZADO: 20k para ver progreso r√°pido

os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)

# ==================== CALLBACK ====================
class TrainingProgressCallback(BaseCallback):
    """Callback para mostrar progreso"""
    
    def __init__(self, check_freq: int = 1000, verbose: int = 1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.episode_rewards = []
        self.episode_lengths = []
        self.successful_episodes = 0
        
    def _on_step(self) -> bool:
        # Capturar episodios completados
        if len(self.model.ep_info_buffer) > 0:
            for info in self.model.ep_info_buffer:
                self.episode_rewards.append(info['r'])
                self.episode_lengths.append(info['l'])
        
        # Mostrar progreso
        if self.n_calls % self.check_freq == 0:
            if len(self.episode_rewards) > 0:
                recent = min(50, len(self.episode_rewards))
                mean_reward = np.mean(self.episode_rewards[-recent:])
                mean_length = np.mean(self.episode_lengths[-recent:])
                max_reward = np.max(self.episode_rewards[-recent:])
                
                print(f"\n{'='*60}")
                print(f"üìä PROGRESO: {self.n_calls}/{TOTAL_TIMESTEPS} steps ({self.n_calls/TOTAL_TIMESTEPS*100:.1f}%)")
                print(f"   Episodios: {len(self.episode_rewards)}")
                print(f"   Recompensa media (√∫ltimos {recent}): {mean_reward:.2f}")
                print(f"   Recompensa m√°xima (√∫ltimos {recent}): {max_reward:.2f}")
                print(f"   Duraci√≥n media: {mean_length:.1f} pasos")
                print(f"{'='*60}\n")
        
        return True

# ==================== ENTORNO ====================
print("="*60)
print("CONFIGURACI√ìN DE ENTRENAMIENTO R√ÅPIDO")
print("="*60)

print("\n1. Conectando con RoboboSim...")
rob = Robobo(ROBOBO_IP)
rbsim = RoboboSim(ROBOBO_IP)

idrobot = 0
idobject = "CYLINDERMIDBALL"

print("2. Creando entorno...")
env = Monitor(
    RoboboEnv(rob, rbsim, idrobot, idobject),
    filename=os.path.join(LOGS_DIR, f"{MODEL_NAME}_monitor")
)

# ==================== MODELO PPO OPTIMIZADO ====================
print("3. Creando modelo PPO (configuraci√≥n r√°pida)...")

model = PPO(
    'MlpPolicy',
    env,
    learning_rate=5e-4,        # ‚Üê AUMENTADO: 5e-4 en lugar de 3e-4
    n_steps=1024,              # ‚Üê REDUCIDO: 1024 en lugar de 2048
    batch_size=64,             # Mantenido
    n_epochs=10,               # Mantenido
    gamma=0.99,                # Mantenido
    gae_lambda=0.95,           # Mantenido
    clip_range=0.2,            # Mantenido
    ent_coef=0.02,             # ‚Üê AUMENTADO: 0.02 para m√°s exploraci√≥n
    vf_coef=0.5,               # Mantenido
    max_grad_norm=0.5,         # Mantenido
    verbose=1,
    tensorboard_log=os.path.join(LOGS_DIR, MODEL_NAME)
)

print("‚úÖ Modelo creado!")
print("\nHiperpar√°metros OPTIMIZADOS para aprendizaje r√°pido:")
print(f"  - Learning rate: 5e-4 (m√°s alto ‚Üí aprende m√°s r√°pido)")
print(f"  - n_steps: 1024 (menos ‚Üí actualiza m√°s frecuente)")
print(f"  - Entrop√≠a: 0.02 (m√°s exploraci√≥n inicial)")
print(f"  - Episodios: m√°x 50 pasos (m√°s cortos)")
print(f"  - Recompensas: m√°s generosas")

# ==================== CALLBACKS ====================
# Checkpoint cada 5k steps
checkpoint_callback = CheckpointCallback(
    save_freq=5_000,
    save_path=os.path.join(MODELS_DIR, "checkpoints"),
    name_prefix=MODEL_NAME,
    save_replay_buffer=False,
    save_vecnormalize=False
)

progress_callback = TrainingProgressCallback(check_freq=1000)

# ==================== ENTRENAMIENTO ====================
print(f"\n{'='*60}")
print(f"INICIANDO ENTRENAMIENTO R√ÅPIDO")
print(f"{'='*60}")
print(f"Total timesteps: {TOTAL_TIMESTEPS:,}")
print(f"Tiempo estimado: 30-60 minutos")
print(f"Checkpoints cada: 5,000 steps")
print(f"Progreso cada: 1,000 steps")
print(f"{'='*60}\n")

try:
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=[checkpoint_callback, progress_callback],
        progress_bar=True,
        reset_num_timesteps=False
    )
    
    print("\n" + "="*60)
    print("‚úÖ ¬°ENTRENAMIENTO COMPLETADO!")
    print("="*60)
    
except KeyboardInterrupt:
    print("\n" + "="*60)
    print("‚ö†Ô∏è  INTERRUMPIDO POR EL USUARIO")
    print("="*60)
    
except Exception as e:
    print(f"\n‚ùå ERROR: {e}")
    import traceback
    traceback.print_exc()
    
finally:
    # Guardar modelo
    model_path = os.path.join(MODELS_DIR, f"{MODEL_NAME}_final")
    print(f"\nGuardando modelo en: {model_path}")
    
    try:
        model.save(model_path)
        print("‚úÖ Modelo guardado")
    except Exception as e:
        print(f"‚ùå Error al guardar: {e}")
    
    # Cerrar
    print("\nCerrando entorno...")
    try:
        env.close()
        print("‚úÖ Cerrado")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error: {e}")

# ==================== INSTRUCCIONES ====================
print(f"\n{'='*60}")
print("PR√ìXIMOS PASOS")
print(f"{'='*60}")
print(f"\n1. Ver m√©tricas:")
print(f"   tensorboard --logdir {LOGS_DIR}")
print(f"\n2. Graficar resultados:")
print(f"   python plot_training_results.py")
print(f"\n3. Validar modelo:")
print(f"   python validate.py")
print(f"\nüí° Si quieres mejor resultado, aumenta TOTAL_TIMESTEPS a 50k-100k")
print(f"{'='*60}\n")